# 最好都能掌握的机器学习数学知识

## 一：概述

学习机器学习和深度学习中，有太多的理论公式推导，没有一定的数学基础是很难理解其意。从另外一个角度上看，数学是机器学习中一个最基础的工具，如果基础都没有打扎实，以后的路将很难走，更别说走远了。
好了，不废话了，请看以下学习清单：

> 事无巨细，我会从某个大方面介绍，其下所包含的额外知识点，请自行脑补，不然这篇文章将会把书籍目录上的知识点都给罗列出来。

## 二：线性代数


### 2.1 标量和向量

标量就是一个数值，不掺杂其它含义。

向量是有大小有方向的量，在这里就是一组有序数，并用以下符号表示：

![标量](./imgs/vector.png)

在实际应用当中，向量通常被当做是高维空间的点，其中每个标量对应具体维度。

### 2.2 矩阵

在介绍矩阵时，我先推荐一款高分视频，这是国外大佬以动画的形式让我们明白，原来线性代数是这个样子的。 [链接](https://space.bilibili.com/88461692?spm_id_from=333.338.v_upinfo.3#/channel/detail?cid=9450)

- 基本运算
    
    对于基本运算，加法、减法、数乘、转置是需要有所了解的。

- 特征值和特征向量

    对于特征值和特征向量，熟悉的人看看公式估计能回忆起来：

        Ax = cx

    ``A``为矩阵，``c`` 为特征值，``x`` 为特征向量。至于计算过程，这不是重点，我接下来要讲的是，特征向量和特征值有什么意义？

    在高维度矩阵中，特征分解可以得到特征值和特征向量，特征值表示的是这个特征到底有多重要，而特征向量表示的是这个特征是什么，可以将每个特征向量理解成一个线性的子空间。[参考链接](http://blog.csdn.net/sunshine_in_moon/article/details/45749691)

    从线性变换的角度来分析，如果矩阵通过变换之后，原向量仍然在变换后的的向量方向上，此方向上的向量就是特征向量。大家可以好好看看[这个](https://www.bilibili.com/video/av6540378?from=search&seid=14733384012190141202)视频，里面很直观的讲解线性变换的几何意义。

    不过，特征分解也有很多局限性，比如说变换的矩阵必须是方阵。


- 线性变换

    通过矩阵基本运算，可以对矩阵进行旋转、拉伸变换，对于三维空间就是翻转，对于更高维度空间，那就自行脑补了~_~~


### 2.3 张量(``tensor``)
看到英文名时应该直接联想到 ``Google`` 的``tensorflow``框架吧。先别急，先看看 ``tensor`` 到底是什么吧。

几何代数中定义的张量是基于向量和矩阵的推广，通俗一点理解的话，我们可以将标量视为零阶张量，矢量视为一阶张量，矩阵就是二阶张量， ``N`` 阶张量就是 ``N`` 维矩阵。

### 2.4 正交分解

特征分解是使用最广的矩阵分解方法之一，

> 矩阵分解是将矩阵拆解为数个矩阵的乘积，可分为三角分解、满秩分解、QR分解、SVD(奇异值)分解等。

### 2.5 奇异值分解(SVD)

奇异值分解（SVD）是在通信系统MIMO、机器学习、图像处理、数据压缩降噪等领域广泛应用的算法。奇异值往往对应着矩阵中隐含的重要信息，且重要性和奇异值大小正相关，每个矩阵A都可以表示为一系列秩为1的“小矩阵”值和，而奇异值则衡量了这些“小矩阵”对于A的权重。公式如下：

![](./imgs/svd.png)

[学习视频](https://www.bilibili.com/video/av15971352?from=search&seid=2357140480380449276) \ [知乎参考链接](https://www.zhihu.com/question/22237507)

在图像处理领域，奇异值不仅可以应用在数据压缩上，还可以对图像去噪，如下图带噪点的数据图：

![噪点数据](./imgs/svd-1.jpg)

通过奇异值分解，我们发现矩阵的奇异值从大到小分别为：14.15、4.67、3.00、0.21 ......，0.05 。 由于奇异值代表着其代表信息的重要程度，通过观察，这里我们只取前三个奇异值，并用其构造新的矩阵，得到：

![取出噪点后的数据](./imgs/svd-2.jpg)

奇异值分解还广泛的用于主成分分析(PCA)和推荐系统等。
